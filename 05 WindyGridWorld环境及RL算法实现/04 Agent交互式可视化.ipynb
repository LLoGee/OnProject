{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90ee686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import Gym_Env\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cdf11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 定义 epsilon 贪心动作选择函数\n",
    "def epsilon_greedy_action(Q, epsilon, state):\n",
    "    if np.random.rand() < 1 - epsilon:\n",
    "        # p(a = a*|s) = 1 - epsilon + epsilon / |A(s)|\n",
    "        action = Action[Q[state].argmax()]\n",
    "    else:\n",
    "        # p(a = a', a'!= a*|s) = epsilon / |A(s)|\n",
    "        action = Action[np.random.choice(range(4))]\n",
    "    return action\n",
    "\n",
    "# 2. 定义数字对应的Action与R_Action，方便动作价值函数 在选择动作时 对应\n",
    "Action = {0:\"up\", 1:\"down\", 2:\"left\", 3:\"right\"}\n",
    "R_Action = {\"up\":0, \"down\":1, \"left\":2, \"right\":3}\n",
    "\n",
    "# 3. 定义Sarsa算法\n",
    "def SARSA(env, num_episodes=300, max_episode_len=1000, gamma=0.9, epsilon=0.1, alpha=0.5):\n",
    "    # 1 初始动作价值函数Q，\n",
    "    # 10,7 表示状态， 4 表明种动作，7行，10列\n",
    "    # 并且保证对于终止所有动作状态，其价值均为0\n",
    "    # Q = np.random.uniform(-1, 0, (7, 10, 4)) # zeros(shape)\n",
    "    # Q[3,7] = np.array([0., 0., 0., 0.])\n",
    "    Q = np.zeros((70,4))\n",
    "    \n",
    "    Reward_Episodes_lis = []\n",
    "    for i in range(num_episodes):\n",
    "        # 记录一下 reward\n",
    "        Total_reward = 0\n",
    "        # 2 初始化状态，对于 gym 环境编写种，每次reset()都会初始化agent位置为(3,0)\n",
    "        state = 30\n",
    "        env.reset()\n",
    "        # 2 选择动作 根据 Q(epsilon-greedy) 进行动作选择\n",
    "        action = epsilon_greedy_action(Q, epsilon, state)\n",
    "\n",
    "        steps = 0\n",
    "        terminated = False\n",
    "        # 终止条件设置为要么步数达到限制，要么agent已经抵达了目标\n",
    "        while (steps <= max_episode_len) & (terminated==False):\n",
    "            steps += 1\n",
    "\n",
    "            # 3 执行动作，并且返还观察结果，奖励，以及是否终止\n",
    "            observation, reward, terminated, _, _ = env.step(action)\n",
    "            Total_reward += reward\n",
    "            \n",
    "            # 3 利用 Q (epsilon-greedy) 进行更新的动作选择，依据上图给出的算法\n",
    "            next_state = observation[\"agent\"][0]*10 + observation[\"agent\"][1]\n",
    "            next_action = epsilon_greedy_action(Q, epsilon, next_state)\n",
    "\n",
    "            # 更新当前 状态与动作 对应的值函数\n",
    "            TD_error = reward + gamma * Q[next_state, R_Action[next_action]] - Q[state, R_Action[action]]\n",
    "            Q[state, R_Action[action]] += alpha * TD_error\n",
    "\n",
    "            # 更新动作与状态\n",
    "            state, action = next_state, next_action\n",
    "        \n",
    "        Reward_Episodes_lis.append(Total_reward)\n",
    "            \n",
    "        #epilon 的探索策略，随着episodes增加，会越来越小，但最小是0.05\n",
    "        epsilon = max(epsilon * 0.99995, 0.05)\n",
    "    \n",
    "    # 算法运行结束后, argmax(1), 表示在第1个维度上最大化，即动作维度上\n",
    "    policy = Q.argmax(1)\n",
    "    \n",
    "    return Q, policy, Reward_Episodes_lis\n",
    "\n",
    "# 4. 初始化环境，并且实用Sarsa算法\n",
    "env = gym.make('WindyGridWorld-v0', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "Q, Policy, Reward_Episodes_lis = SARSA(env)\n",
    "\n",
    "# 5. 将Policy转换为容易理解的表格(对应环境Grid图)\n",
    "TAction = {0:\"up\", 1:\"down\", 2:\"left\", 3:\"right\"}\n",
    "replace_func = np.vectorize(lambda x:TAction.get(x, x))\n",
    "Policy_for_view = replace_func(Policy.reshape(7,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdba12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['left', 'up', 'right', 'right', 'right', 'right', 'right',\n",
       "        'right', 'right', 'down'],\n",
       "       ['left', 'left', 'right', 'right', 'right', 'right', 'up', 'up',\n",
       "        'right', 'down'],\n",
       "       ['up', 'right', 'right', 'up', 'up', 'right', 'up', 'down',\n",
       "        'right', 'down'],\n",
       "       ['right', 'right', 'right', 'right', 'right', 'right', 'right',\n",
       "        'up', 'left', 'down'],\n",
       "       ['right', 'right', 'right', 'right', 'right', 'right', 'up',\n",
       "        'down', 'left', 'left'],\n",
       "       ['down', 'right', 'right', 'right', 'right', 'up', 'up', 'right',\n",
       "        'left', 'down'],\n",
       "       ['down', 'right', 'right', 'right', 'up', 'up', 'up', 'up', 'up',\n",
       "        'left']], dtype='<U5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Policy_for_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061b52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Policy, epsilon, state):\n",
    "    if np.random.rand() < 1 - epsilon:\n",
    "        # p(a = a*|s) = 1 - epsilon + epsilon / |A(s)|\n",
    "        action = Action[Q[state].argmax()]\n",
    "    else:\n",
    "        # p(a = a', a'!= a*|s) = epsilon / |A(s)|\n",
    "        action = Action[np.random.choice(range(4))]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606daa1",
   "metadata": {},
   "source": [
    "# -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81193d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = gym.make('WindyGridWorld-v0', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea77f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsiodes = 30\n",
    "for i in range(epsiodes):\n",
    "    observation,_ = env_test.reset()\n",
    "    state = observation[\"agent\"][0]*10 + observation[\"agent\"][1]\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        state = observation[\"agent\"][0]*10 + observation[\"agent\"][1]\n",
    "        action = epsilon_greedy_policy(Policy, 0.05, state)\n",
    "        observation, reward, terminated, _, _ = env_test.step(action)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207516bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
