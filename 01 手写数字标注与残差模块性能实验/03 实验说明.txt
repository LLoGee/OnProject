该项目旨在探究Residual module的影响因素及其性能变化
	数据集为手写符号，0-6，a-h, A-H 除开字母c，总共 18类，每一类有80张手动书写并标注的图片
		为了保证数据多样性，每类图片尽量采用不同的方式书写，且存在阴影干扰
	为了保证该项目的执行难度，随即划分数据集为 5：5

================


1 初始baseline基准模型在测试集上的效果为
	Test data | Accuracy: 72.29 %, Loss: 65.5122 
		注意：该模型在 mnist 上同样的数据处理与划分，取得的结果为 99%的正确率
			表明该基准模型是有效的

2 在不改动数据集，只针对baseline进行Residual module添加与修改实验
	修改部分有
		Residual module 通道数量，即宽度
		每个通道的卷积核的尺寸
		每个通道卷积核的数量

3 最终实验结果

	1. 在原数据后增加残差连接结构不仅不会提升网络性能，反而干扰数据，使表现更糟糕
		1.1 数据必须经过激活函数后相加，否则可能是造成干扰
	2. 正常情况下增加残差连接结构都能提升网络性能
	3. 如果只添加一层残差连接，那么越靠近后面提升的效果越明显
	4. 缺点是参数量大大提升，网络训练速度慢
	5. 对于简单的分类任务来说，过多的残差连接也可能导致过拟合的情况
	6. 可以加宽残差连接层，从而提升性能，
		如果加太多的话，就会超过模型复杂度上限，模型性能下降


================


Trick: 
	1. 如果只想提升网络的精度，不考虑参数量与推理速度，
    		都可以堆叠多个残差结构，但注意激活函数的使用顺序

	2. 如果有考虑网络参数量，又想提升精度，可以尝试只在最后一层添加残差结构
	    	这样对比全部都添加残差结构，正确率只损失了2%，不会增加太多参数，又能保证性能大幅提升

final：
	最终实验：4（即基准模型的第三层卷积层之后添加残差模块）并且加宽7倍的效果最好
		Test data | Accuracy: 94.29 %, Loss: 6.6065 

感触：深度学习是一门十分注重实验的学科。我们需要又创新的想法，但同时也需要大量的实验来证明我们的想法