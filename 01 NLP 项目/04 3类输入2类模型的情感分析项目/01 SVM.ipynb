{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 与 GoogleDrive 链接，轻松导入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20413,
     "status": "ok",
     "timestamp": 1684241211559,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "xO8nRhdVxRNt",
    "outputId": "b0f17a20-a172-4238-dbab-a6954b9ded09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n",
      "/content/drive/MyDrive/2023NLPCourse/Assignment1/Part_A\n"
     ]
    }
   ],
   "source": [
    "# 下面两步是如何链接colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# 通常，直接给出路径然后导入即可\n",
    "# 另外，%cd命令到该路径，然后按文件名导入\n",
    "\n",
    "dir_path = '/content/drive/MyDrive/2023NLPCourse/Assignment1/Part_A'\n",
    "file_name = 'IMDB Dataset.csv'\n",
    "\n",
    "# cd 到该路径下\n",
    "%cd /content/drive/MyDrive/2023NLPCourse/Assignment1/Part_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据导入和数据标签处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3491,
     "status": "ok",
     "timestamp": 1684241215043,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "Jj3cr9vpx3zE",
    "outputId": "1312db84-5bac-45fc-ab6f-12c63b06764f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-08b8c9f706b6>:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(\"Data/\"+file_name, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Data/\"+file_name, error_bad_lines=False)\n",
    "data['label'] = data['sentiment'].replace(['positive', 'negative'],['1', '0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.定义预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12098,
     "status": "ok",
     "timestamp": 1683877264978,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "flC-UajL3gk2",
    "outputId": "3e708736-7297-4a51-b669-3c09266de87a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "# 用于情感分析的文本预处理\n",
    "import string\n",
    "import emoji\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    # 1. 生成推文中的单词列表（已删除标签和其他标点符号）\n",
    "    text_blob = TextBlob(text) # 生成 textblob对象\n",
    "    text = ' '.join(text_blob.words) # 空格连接textblob对象识别到的词语，即连接成整段文本，且去除其他符号\n",
    "    \n",
    "    # 2. re模块清理数字符号\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    \n",
    "    # 3. 全部小写化\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 4. emoji模块将表情转换为文本\n",
    "    text = emoji.demojize(text)\n",
    "    \n",
    "    # 5. 对于一些仍有可能存在的标点符号去除，使用string模块 \n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    \n",
    "    # 6. 进行分词处理\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    # 7. 移除可能存在的空白token\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    \n",
    "    # 8. 移除一些非英文字母的token\n",
    "    text = [t for t in text if t.isalpha()]\n",
    "    \n",
    "    # 9. 替换否定标记，便于感情识别\n",
    "    replacer  = AntonymReplacer()\n",
    "    text = replacer.replace_negations(text)\n",
    "    \n",
    "    # 10. 移除无用的停用词\n",
    "    text = [i for i in text if i not in stopwords]\n",
    "    \n",
    "    # 11. 最后进行词干提取\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    text = [porter_stemmer.stem(w) for w in text]\n",
    "    \n",
    "    return text\n",
    "\n",
    "class AntonymReplacer(object):\n",
    "    def replace(self, word, pos=None):\n",
    "        antonyms = set()\n",
    "\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonyms.add(antonym.name())\n",
    "\n",
    "        if len(antonyms) == 1:\n",
    "            return antonyms.pop()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def replace_negations(self, sent):\n",
    "        i, l = 0, len(sent)\n",
    "        words = []\n",
    "\n",
    "        while i < l:\n",
    "            word = sent[i]\n",
    "\n",
    "            if word == 'not' and i+1 < l:\n",
    "                ant = self.replace(sent[i+1])\n",
    "\n",
    "                if ant:\n",
    "                    words.append(ant)\n",
    "                    i += 2\n",
    "                    continue\n",
    "\n",
    "            words.append(word)\n",
    "            i += 1\n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 导入 CountVectorizer & TfidfVectorizer 模型来生成对应特征表示向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UT0D01iYz-MO"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# Fit：在数据上训练模型，在训练期间使用 3 中定义好的预处理函数\n",
    "# Transform：最后用模型对数据集生成对应的向量数据\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "Count_vectorizer = CountVectorizer(analyzer=preprocess)\n",
    "Tfidf_vectorizer = TfidfVectorizer(analyzer=preprocess, min_df=2, max_df=0.9, sublinear_tf=True, use_idf=True)\n",
    "\n",
    "Count_model = Count_vectorizer.fit(data['review'])\n",
    "Tfidf_model = Tfidf_vectorizer.fit(data['review'])\n",
    "\n",
    "Count_data = Count_model.transform(data['review'])\n",
    "Tfidf_data = Tfidf_model.transform(data['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 保存模型与数据到对应的文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4fiUQ3z4rc3"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(Count_model, open('Models/Count_model.save', 'wb'))\n",
    "pickle.dump(Tfidf_model, open('Models/Tfidf_model.save', 'wb'))\n",
    "\n",
    "pickle.dump(Count_data, open('Data/Count_data.save', 'wb'))\n",
    "pickle.dump(Tfidf_data, open('Data/Tfidf_data.save', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 分割数据集并训练两个SVM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6203955,
     "status": "ok",
     "timestamp": 1683887054878,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "4LbCzMbvCr9t",
    "outputId": "65f0ce7f-9de4-45aa-df21-2c0cf7d8b804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of {}\n",
      "SVC()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88      4761\n",
      "           1       0.90      0.86      0.88      5239\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[4280  481]\n",
      " [ 726 4513]]\n",
      "\n",
      "\n",
      "Accuracy score:  0.8793\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Result of {}\n",
      "SVC()\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      4875\n",
      "           1       0.91      0.89      0.90      5125\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "\n",
      "\n",
      "Confusion matrix: \n",
      " [[4435  440]\n",
      " [ 571 4554]]\n",
      "\n",
      "\n",
      "Accuracy score:  0.8989\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(Data, model):\n",
    "    SEED = 4000\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Data, data.label, test_size=0.2, random_state=SEED)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    print('Result of {}\\n'+str(model))\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(predictions, y_test))\n",
    "    print('\\n')\n",
    "    print('Confusion matrix: \\n', confusion_matrix(predictions, y_test))\n",
    "    print('\\n')\n",
    "    print('Accuracy score: ', accuracy_score(predictions, y_test))\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "SVM_CountVectorizer = svm.SVC()\n",
    "SVM_TfidfVectorizer = svm.SVC()\n",
    "\n",
    "train(Count_data, SVM_CountVectorizer)\n",
    "train(Tfidf_data, SVM_TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 保存两个 SVM 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Xi0z_qfHrJW"
   },
   "outputs": [],
   "source": [
    "pickle.dump(SVM_CountVectorizer, open('Models/SVM_CountVectorizer.save', 'wb'))\n",
    "pickle.dump(SVM_TfidfVectorizer, open('Models/SVM_TfidfVectorizer.save', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN7KO50u8EhOwVObaf1D99/",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
