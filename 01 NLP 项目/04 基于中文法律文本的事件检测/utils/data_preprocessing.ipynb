{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Link to colab drive & import packages\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","dir_path = '/content/drive/MyDrive/Project/'\n","\n","%cd /content/drive/MyDrive/Project/\n","\n","import os\n","import json\n","import copy\n","import codecs\n","import numpy as np\n","from tqdm import tqdm\n","import pickle\n","# from utils.global_variables import Global"],"metadata":{"id":"Y_WkpZVAL2ze","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697979376207,"user_tz":-480,"elapsed":19508,"user":{"displayName":"SK TIAAN","userId":"01952584446737432247"}},"outputId":"e903862d-0940-4ba3-89a3-5c0bbf3d7d4d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Project\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pkcdAzb0TqgM"},"outputs":[],"source":["class LevenReader(object):\n","    def __init__(self):\n","        self.data = []\n","        self.raw_dir = \"./raw\"\n","        self.data_dir = \"./data\"\n","        self.flag_dir = ['Data', 'Data_Crf']\n","        self.word2vec_source_file = 'sgns.baidubaike.bigram-char'\n","        self.word2vec_file = \"word2vec.npy\"\n","        self.modes = [\"train\", \"valid\"]\n","        self.embedding_dict = self.load_embedding_dict(os.path.join(self.raw_dir, self.word2vec_source_file))\n","        self.vocab_size = 0\n","        self.embedding_size = 0\n","\n","    def read(self, crf=0):\n","        \"\"\"\n","        :param mode: train/valid/test\n","        :return: [{\"tokens\": list(int), \"labels\": list(int)}, ...]\n","        \"\"\"\n","        for mode in self.modes:\n","          self.data.clear()\n","          if not os.path.exists(os.path.join(self.data_dir, self.flag_dir[crf], 'flag')):\n","              os.makedirs(os.path.join(self.data_dir, self.flag_dir[crf]))\n","              self.preprocess(crf)\n","          with open(os.path.join(self.data_dir, self.flag_dir[crf], \"{}_processed.json\".format(mode)), \"r+\", encoding=\"utf-8\") as f:\n","              data = json.load(f)\n","          if crf==0 and mode=='train':\n","            Global_dict = {}\n","            Global_dict[\"word2id\"] = data[\"word2id\"]\n","            Global_dict[\"id2word\"] = data[\"id2word\"]\n","            Global_dict[\"label2id\"] = data[\"label2id\"]\n","            Global_dict[\"id2label\"] = data[\"id2label\"]\n","            with open(os.path.join(self.data_dir, self.flag_dir[crf], f'Global_dict_crf_{crf}.pkl'), 'wb') as file:\n","              pickle.dump(Global_dict, file)\n","          if crf==1 and mode=='train':\n","            Global_dict = {}\n","            Global_dict[\"word2id\"] = data[\"word2id\"]\n","            Global_dict[\"id2word\"] = data[\"id2word\"]\n","            Global_dict[\"label2id\"] = data[\"label2id\"]\n","            Global_dict[\"id2label\"] = data[\"id2label\"]\n","            Global_dict[\"type2id\"] = data[\"type2id\"]\n","            with open(os.path.join(self.data_dir, self.flag_dir[crf], f'Global_dict_crf_{crf}.pkl'), 'wb') as file:\n","              pickle.dump(Global_dict, file)\n","\n","          for item in data[\"info\"]:\n","              tokens = [data[\"word2id\"][x] if x in data[\"word2id\"] else data[\"word2id\"][\"<UNK>\"] for x in item[\"tokens\"]]\n","              if mode != \"test\":\n","                  labels = [data[\"label2id\"][x] for x in item[\"labels\"]]\n","              canids = item[\"canids\"]\n","              docids = item[\"docids\"]\n","              if crf==0:\n","                  for i in range(len(canids)):\n","                      if item[\"flags\"][i]:\n","                          if mode != \"test\":\n","                              temp = {\"tokens\": tokens,\n","                                      \"labels\": labels[i],\n","                                      \"canids\": canids[i],\n","                                      \"docids\": docids,\n","                                      \"index\": i}\n","                          else:\n","                              temp = {\"tokens\": tokens,\n","                                      \"canids\": canids[i],\n","                                      \"docids\": docids,\n","                                      \"index\": i}\n","                          self.data.append(temp)\n","              else:\n","                  if mode != \"test\":\n","                      temp = {\"tokens\": tokens,\n","                              \"labels\": labels,\n","                              \"canids\": canids,\n","                              \"docids\": docids,\n","                              \"flags\": item[\"flags\"]}\n","                  else:\n","                      temp = {\"tokens\": tokens,\n","                              \"canids\": canids,\n","                              \"docids\": docids,\n","                              \"flags\": item[\"flags\"]}\n","                  self.data.append(temp)\n","\n","          if mode=='train':\n","            config_runtime_dict = {}\n","            config_runtime_dict[\"vocab_size\"] = str(self.vocab_size)\n","            config_runtime_dict[\"embedding_size\"] = str(self.embedding_size)\n","            config_runtime_dict[\"num_class\"] = str(len(data[\"label2id\"]))\n","            config_runtime_dict[\"sequence_length\"] = str(data[\"sequence_length\"])\n","            with open(os.path.join(self.data_dir, self.flag_dir[crf], f'config_runtime_dict_crf_{crf}.pkl'), 'wb') as file:\n","              pickle.dump(config_runtime_dict, file)\n","\n","\n","          print(\"Mode: {} | Dataset Size = {}\".format(mode, len(self.data)))\n","          with open(os.path.join(self.data_dir, self.flag_dir[crf], f'{mode}_data.pkl'), 'wb') as file:\n","            pickle.dump(copy.deepcopy(self.data), file)\n","\n","    def preprocess(self, crf=0):\n","        \"\"\"\n","        :return: output file, integrated data and word vector matrix\n","         Integrate data formatsï¼š{\n","            \"info\":[{\"tokens\": list(str), \"labels\": list(str), \"flags\": list(bool)}, ...],\n","            \"word2id\": {\"<PAD>\": 0, \"<UNK>\": 1},\n","            \"id2word\": {0: \"<PAD>\", 1: \"<UNK>\"},\n","            \"label2id\": {\"None\": 0},\n","            \"id2label\": {0: \"None\"},\n","            \"sequence_length\": int\n","        }\n","        \"\"\"\n","\n","        processed_data = {\"info_train\": [],\n","                                          \"info_valid\": [],\n","                                          \"info_test\": [],\n","                                          \"word2id\": {},\n","                                          \"id2word\": {},\n","                                          \"label2id\": {},\n","                                          \"id2label\": {},\n","                                          \"sequence_length\": 0}\n","\n","        if crf==1:\n","            processed_data[\"label2id\"][\"O\"] = 0\n","            processed_data[\"id2label\"][0] = \"O\"\n","            processed_data[\"type2id\"] = {\"O\": 0}\n","        else:\n","            processed_data[\"label2id\"][\"None\"] = 0\n","            processed_data[\"id2label\"][0] = \"None\"\n","\n","        for mode in self.modes:\n","            with codecs.open(os.path.join(self.raw_dir, \"{}.jsonl\".format(mode)), 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n","                lines = f.readlines()\n","                for line in lines:\n","                    line = line.rstrip()\n","                    doc = json.loads(line)\n","                    docids = doc[\"id\"]\n","                    doc_tokens, doc_labels, doc_canids, doc_flags = [], [], [], []\n","                    for item in doc[\"content\"]:\n","                        doc_tokens.append(item[\"tokens\"])\n","\n","                    if crf==1:\n","                        for tokens in doc_tokens:\n","                            if mode != \"test\":\n","                                doc_labels.append([\"O\"] * len(tokens))\n","                            doc_canids.append([\"\"] * len(tokens))\n","                            doc_flags.append([0] * len(tokens))\n","\n","                        if mode == \"test\":\n","                            for candi in doc[\"candidates\"]:\n","                                for i in range(candi[\"offset\"][0], candi[\"offset\"][1]):\n","                                    doc_canids[candi[\"sent_id\"]][i] = candi[\"id\"]\n","                                    doc_flags[candi[\"sent_id\"]][i] = 1\n","                        else:\n","                            for event in doc[\"events\"]:\n","                                tp = event[\"type\"].replace(\"-\", \"_\")\n","                                if tp not in processed_data[\"type2id\"]:\n","                                    processed_data[\"type2id\"][tp] = event[\"type_id\"]\n","                                for mention in event[\"mention\"]:\n","                                    for i in range(mention[\"offset\"][0], mention[\"offset\"][1]):\n","                                        doc_labels[mention[\"sent_id\"]][i] = (\"B-\" + tp) if (i == mention[\"offset\"][0]) else (\"I-\" + tp)\n","                                        doc_canids[mention[\"sent_id\"]][i] = mention[\"id\"]\n","                                        doc_flags[mention[\"sent_id\"]][i] = 1\n","\n","                    else:\n","                        for tokens in doc_tokens:\n","                            if mode != \"test\":\n","                                doc_labels.append([\"None\"] * len(tokens))\n","                            doc_canids.append([\"\"] * len(tokens))\n","                            doc_flags.append([0] * len(tokens))\n","                            processed_data[\"sequence_length\"] = max(processed_data[\"sequence_length\"], len(tokens))\n","\n","                        if mode == \"test\":\n","                            for candi in doc[\"candidates\"]:\n","                                for i in range(candi[\"offset\"][0], candi[\"offset\"][1]):\n","                                    doc_canids[candi[\"sent_id\"]][i] = candi[\"id\"]\n","                                    doc_flags[candi[\"sent_id\"]][i] = 1\n","                        else:\n","                            for event in doc[\"events\"]:\n","                                if event[\"type\"] not in processed_data[\"label2id\"]:\n","                                    processed_data[\"label2id\"][event[\"type\"]] = event[\"type_id\"]\n","                                    processed_data[\"id2label\"][event[\"type_id\"]] = event[\"type\"]\n","                                for mention in event[\"mention\"]:\n","                                    for i in range(mention[\"offset\"][0], mention[\"offset\"][1]):\n","                                        doc_labels[mention[\"sent_id\"]][i] = event[\"type\"]\n","                                        doc_canids[mention[\"sent_id\"]][i] = mention[\"id\"]\n","                                        doc_flags[mention[\"sent_id\"]][i] = 1\n","\n","                    if mode != \"test\":\n","                        for mention in doc[\"negative_triggers\"]:\n","                            for i in range(mention[\"offset\"][0], mention[\"offset\"][1]):\n","                                doc_canids[mention[\"sent_id\"]][i] = mention[\"id\"]\n","                                doc_flags[mention[\"sent_id\"]][i] = 1\n","\n","                        for tokens, labels, canids, flags in zip(doc_tokens, doc_labels, doc_canids, doc_flags):\n","                            processed_data[\"info_{}\".format(mode)].append({\"tokens\": tokens,\n","                                                                           \"labels\": labels,\n","                                                                           \"canids\": canids,\n","                                                                           \"flags\": flags,\n","                                                                           \"docids\": docids})\n","                            if crf==1:\n","                                for label in labels:\n","                                    if label not in processed_data[\"label2id\"]:\n","                                        id = len(processed_data[\"label2id\"])\n","                                        processed_data[\"label2id\"][label] = id\n","                                        processed_data[\"id2label\"][id] = label\n","                    else:\n","                        for tokens, canids, flags in zip(doc_tokens, doc_canids, doc_flags):\n","                            processed_data[\"info_{}\".format(mode)].append({\"tokens\": tokens,\n","                                                                           \"canids\": canids,\n","                                                                           \"flags\": flags,\n","                                                                           \"docids\": docids})\n","\n","        if crf==1:\n","            processed_data[\"sequence_length\"] = 512\n","\n","        word2vec_mat = []\n","        for (k, v) in tqdm(self.embedding_dict.items(), desc='reading pretrained word embeddings'):\n","            id = len(processed_data[\"word2id\"])\n","            processed_data[\"word2id\"][k] = id\n","            processed_data[\"id2word\"][id] = k\n","            word2vec_mat.append(v)\n","\n","        word2vec_mat = np.array(word2vec_mat, dtype=np.float32)\n","        self.vocab_size = word2vec_mat.shape[0]\n","        self.embedding_size = word2vec_mat.shape[1]\n","        if not os.path.exists(os.path.join(self.data_dir, self.word2vec_file)):\n","            np.save(os.path.join(self.data_dir, self.word2vec_file), word2vec_mat)\n","\n","        for mode in self.modes:\n","            with open(os.path.join(self.data_dir, self.flag_dir[crf], \"{}_processed.json\".format(mode)), \"w\", encoding=\"utf-8\") as f:\n","                temp_data = {\"info\": processed_data[\"info_{}\".format(mode)],\n","                                          \"word2id\": processed_data[\"word2id\"],\n","                                          \"id2word\": processed_data[\"id2word\"],\n","                                          \"label2id\": processed_data[\"label2id\"],\n","                                          \"id2label\": processed_data[\"id2label\"],\n","                                          \"sequence_length\": processed_data[\"sequence_length\"]}\n","                if crf==1:\n","                    temp_data[\"type2id\"] = processed_data[\"type2id\"]\n","                json.dump(temp_data, f, indent=2, ensure_ascii=False)\n","\n","        with open(os.path.join(self.data_dir, self.flag_dir[crf], 'flag'), \"w+\") as f:\n","            f.write(\"\")\n","\n","    @staticmethod\n","    def load_embedding_dict(path):\n","        lines = open(path, encoding='utf-8').readlines()\n","        embedding_dict = {}\n","        for i, line in enumerate(lines):\n","            if i == 0 and '\\n' in line:\n","                continue\n","\n","            if '\\n' in line:\n","                line = line[:-2]    # remove the '[blank]\\n' in the end of the string\n","\n","            split = line.split(\" \")\n","            embedding_dict[split[0]] = np.array(list(map(float, split[1:])))\n","\n","        unk = sum(list(embedding_dict.values())) / len(embedding_dict.keys())\n","        embedding_dict['<UNK>'] = unk\n","        embedding_dict['<PAD>'] = np.random.randn(unk.shape[0])\n","        return embedding_dict"]},{"cell_type":"code","source":["reader= LevenReader()"],"metadata":{"id":"uN60kcrAm1bB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reader.read(crf=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rp6_HV4BnalC","executionInfo":{"status":"ok","timestamp":1697596806775,"user_tz":-480,"elapsed":42638,"user":{"displayName":"SK TIAAN","userId":"01952584446737432247"}},"outputId":"8e624378-c075-4a6b-e089-5e5b953371ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["reading pretrained word embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 635976/635976 [00:00<00:00, 971759.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Mode: train | Dataset Size = 395322\n","Mode: valid | Dataset Size = 92451\n"]}]},{"cell_type":"code","source":["reader.read(crf=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53QpYrjzm-v4","executionInfo":{"status":"ok","timestamp":1697596879747,"user_tz":-480,"elapsed":36135,"user":{"displayName":"SK TIAAN","userId":"01952584446737432247"}},"outputId":"0ba912bf-02dd-4bf7-ee33-651022906bc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["reading pretrained word embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 635976/635976 [00:00<00:00, 966625.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Mode: train | Dataset Size = 41238\n","Mode: valid | Dataset Size = 9788\n"]}]},{"cell_type":"code","source":["# get the lable counts\n","def get_labels_dict(path):\n","  with open(path, 'rb') as file:\n","    loaded_data = pickle.load(file)\n","  frequency_dict = {}\n","  if 'Crf' not in path:\n","    for data in loaded_data:\n","        label = data['labels']\n","        if label in frequency_dict:\n","            frequency_dict[label] += 1\n","        else:\n","            frequency_dict[label] = 1\n","  else:\n","      for data in loaded_data:\n","        labels = data['labels']\n","        for label in labels:\n","          if label in frequency_dict:\n","              frequency_dict[label] += 1\n","          else:\n","              frequency_dict[label] = 1\n","  sorted_dict = dict(sorted(frequency_dict.items()))\n","  df = pd.DataFrame.from_dict(sorted_dict,orient='index',columns=['count'])\n","  df.to_csv(path[:-4]+'_label_distribution.csv')\n","\n","\n","data_path_train = './data/Data/train_data.pkl'\n","data_path_valid = './data/Data/valid_data.pkl'\n","datacrf_path_train = './data/Data_Crf/train_data.pkl'\n","datacrf_path_valid = './data/Data_Crf/valid_data.pkl'\n","\n","get_labels_dict(data_path_train)\n","get_labels_dict(data_path_valid)\n","get_labels_dict(datacrf_path_train)\n","get_labels_dict(datacrf_path_valid)"],"metadata":{"id":"pJw0ER2Sreap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get sentence lengths distribution\n","import pickle\n","import pandas as pd\n","with open(  './data/Data_Crf/train_data.pkl', 'rb') as file:\n","  loaded_data = pickle.load(file)\n","  sentence_lengths = [len(data['tokens']) for data in loaded_data]\n","  df = pd.DataFrame(sentence_lengths, columns=[\"lengths\"])\n","  df.to_csv(\"./data/sentence_lengths.csv\", index=False)"],"metadata":{"id":"4iLMEDCwc7cC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HgX9pwkPc7Qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9fdH9Ytqc6gy","executionInfo":{"status":"ok","timestamp":1697979647058,"user_tz":-480,"elapsed":708,"user":{"displayName":"SK TIAAN","userId":"01952584446737432247"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BqrEhfnfdw8k"},"execution_count":null,"outputs":[]}]}