{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1870ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "import Gym_Env\n",
    "\n",
    "def epsilon_greedy_action(Q, epsilon, state):\n",
    "    if np.random.rand() < 1 - epsilon:\n",
    "        # p(a = a*|s) = 1 - epsilon + epsilon / |A(s)|\n",
    "        action = Action[Q[state[0],state[1]].argmax()]\n",
    "    else:\n",
    "        # p(a = a', a'!= a*|s) = epsilon / |A(s)|\n",
    "        action = Action[np.random.choice(range(4))]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce43fc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数字对应的Action与R_Action，方便动作价值函数 在选择动作时 对应\n",
    "Action = {0:\"up\", 1:\"down\", 2:\"left\", 3:\"right\"}\n",
    "R_Action = {\"up\":0, \"down\":1, \"left\":2, \"right\":3}\n",
    "\n",
    "def SARSA(env, num_episodes=500, max_episode_len=2000, gamma=0.9, epsilon=0.1, alpha=0.5):\n",
    "    # 1 初始动作价值函数Q，\n",
    "    # 10,7 表示状态， 4 表明种动作，7行，10列\n",
    "    # Q = np.random.uniform(-1, 0, (7, 10, 4)) # zeros(shape)\n",
    "    # Q[3,7] = np.array([0., 0., 0., 0.])\n",
    "    \n",
    "    Q = np.zeros((7,10,4))\n",
    "    \n",
    "    reward_lis = []\n",
    "    for i in range(num_episodes):\n",
    "        # 记录一下 reward\n",
    "        R = 0\n",
    "        # 2 初始化状态，对于 gym 环境编写种，每次reset()都会初始化agent位置为(0,3)\n",
    "        state = np.array([3, 0])\n",
    "        env.reset()\n",
    "        # 2 选择动作 根据 Q(epsilon-greedy) 进行动作选择\n",
    "        action = epsilon_greedy_action(Q, epsilon, state)\n",
    "\n",
    "        steps = 0\n",
    "        terminated = False\n",
    "        # 终止条件设置为要么步数达到限制，要么agent已经抵达了目标\n",
    "        while (steps <= max_episode_len) & (terminated==False):\n",
    "            steps += 1\n",
    "\n",
    "            # 3 执行动作，并且返还观察结果，奖励，以及是否终止\n",
    "            observation, reward, terminated, _, _ = env.step(action)\n",
    "            R += reward\n",
    "            \n",
    "            if terminated == True:\n",
    "                print(\"finished\")\n",
    "            \n",
    "            # 3 利用 Q (epsilon-greedy) 进行更新的动作选择，依据上图给出的算法\n",
    "            next_state = observation[\"agent\"]\n",
    "            next_action = epsilon_greedy_action(Q, epsilon, next_state)\n",
    "\n",
    "            # 更新当前 状态与动作 对应的值函数\n",
    "            TD_error = reward + gamma * Q[next_state[0], next_state[1], R_Action[next_action]] - Q[state[0], state[1], R_Action[action]]\n",
    "            Q[state[0], state[1], R_Action[action]] += alpha * TD_error\n",
    "\n",
    "            # 更新动作与状态\n",
    "            state, action = next_state, next_action\n",
    "        \n",
    "        reward_lis.append(R)\n",
    "            \n",
    "        #epilon 的探索策略，随着episodes增加，会越来越小，但最小是0.05\n",
    "        epsilon = max(epsilon * 0.99995, 0.05)\n",
    "    \n",
    "    # 算法运行结束后, argmax(2), 表示在第二个维度上最大化，即动作维度上\n",
    "    policy = Q.argmax(2)\n",
    "    \n",
    "    return Q, policy, reward_lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 个人目前不太能理解，为何对于状态空间转换为矩阵时，会出现错误\n",
    "# 将每一个状态转换为标签，最终能够正确执行代码"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
