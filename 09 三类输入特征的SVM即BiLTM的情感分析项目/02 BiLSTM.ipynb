{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 与 GoogleDrive 链接，轻松导入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3966,
     "status": "ok",
     "timestamp": 1684246496264,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "SHzmT0TvlsN6",
    "outputId": "f63ca8c8-4425-46d7-d2b4-f8e41703c437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
      "/content/drive/MyDrive/2023NLPCourse/Assignment1/Part_A\n"
     ]
    }
   ],
   "source": [
    "# 下面两步是如何链接colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# 通常，直接给出路径然后导入即可\n",
    "# 另外，%cd命令到该路径，然后按文件名导入\n",
    "\n",
    "dir_path = '/content/drive/MyDrive/2023NLPCourse/Assignment1/Part_A'\n",
    "file_name = 'IMDB Dataset.csv'\n",
    "\n",
    "# cd 到该路径下\n",
    "%cd /content/drive/MyDrive/2023NLPCourse/Assignment1/Part_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.数据导入和数据标签处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1684246496855,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "MAW4YoFjlyS_",
    "outputId": "8fbe284b-4ddd-4f19-822b-25279a190ff0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-08b8c9f706b6>:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(\"Data/\"+file_name, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Data/\"+file_name, error_bad_lines=False)\n",
    "data['label'] = data['sentiment'].replace(['positive', 'negative'],['1', '0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 定义相关的超参数 并训练 Word2Vec 等模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8627,
     "status": "ok",
     "timestamp": 1684246505479,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "qjuWWV_cmI9K",
    "outputId": "23ce5eee-192a-46c5-aeb8-adfdfdbcb526"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Word2Vec 超参数\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "# keras 模型超参数\n",
    "SEQUENCE_LENGTH = 80\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# 情感参数\n",
    "POSITIVE = \"positive\"\n",
    "NEGATIVE = \"negative\"\n",
    "NEUTRAL = \"neutral\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7) # 调整情绪阈值，得分0.4以下才为消极，得分0.7以上则为积极\n",
    "\n",
    "target_cnt = Counter(data.sentiment)\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# 简单的文本与处理函数，只是清楚一些无关的字符，移除停用词，词干提取\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "def preprocess(text, stem=False):\n",
    "    # 删除链接、用户和特殊字符\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.数据预处理并将数据分割到训练测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22628,
     "status": "ok",
     "timestamp": 1684246528104,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "qfhvX7qPo-I-",
    "outputId": "52a261b6-c21e-4303-f18d-7891f0cac0c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 s, sys: 251 ms, total: 21.2 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data.review = data.review.apply(lambda x: preprocess(x))\n",
    "\n",
    "# split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data['review'].values\n",
    "y = data['sentiment'].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "documents = [_text.split() for _text in data.review]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 使用gensium的模块训练word2vec模型（CBOW和Skip-gram）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKIHz723pOHa"
   },
   "outputs": [],
   "source": [
    "# CBOW\n",
    "CBOW_w2vmodel = gensim.models.Word2Vec(documents, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8, sg=0)\n",
    "\n",
    "# Skip-gram\n",
    "Skipgram_w2vmodel = gensim.models.Word2Vec(documents, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. transform数据作为 BiLSTIM 模型的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kwd9UtKIqPd5"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.review)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# 限定文本的长度，最大长度为 80 个word\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=80)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=80)\n",
    "\n",
    "labels = data.sentiment.unique().tolist()\n",
    "labels.append(\"neutral\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(data.sentiment.tolist())\n",
    "\n",
    "y_train = encoder.transform(Y_train.tolist()) \n",
    "y_test = encoder.transform(Y_test.tolist())\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 从经过训练的 word2vec 模型获取 WordEmedding 矩阵(目的是将参数导入到tensorflow的bilstm模型的embedding层中)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4p2bZlew0OA"
   },
   "outputs": [],
   "source": [
    "CBOW_embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "Skipgram_embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in CBOW_w2vmodel.wv:\n",
    "        CBOW_embedding_matrix[i] = CBOW_w2vmodel.wv[word]\n",
    "    if word in Skipgram_w2vmodel.wv:\n",
    "        Skipgram_embedding_matrix[i] = CBOW_w2vmodel.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.基于两个不同的词嵌入矩阵，构建两个不同的Bilstm模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fqnf2G7iyCCI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# CBOW_BiLSTM model\n",
    "CBOW_BiLSTM_model = Sequential()\n",
    "CBOW_BiLSTM_model.add(Embedding(vocab_size, W2V_SIZE, weights=[CBOW_embedding_matrix], \n",
    "                                input_length=SEQUENCE_LENGTH, trainable=False))\n",
    "CBOW_BiLSTM_model.add(Dropout(0.5))\n",
    "CBOW_BiLSTM_model.add(Bidirectional(LSTM(100, dropout=0.2)))\n",
    "CBOW_BiLSTM_model.add(Dense(1, activation='sigmoid'))\n",
    "CBOW_BiLSTM_model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# Skipgram_BiLSTM model\n",
    "Skipgram_BiLSTM_model = Sequential()\n",
    "Skipgram_BiLSTM_model.add(Embedding(vocab_size, W2V_SIZE, weights=[Skipgram_embedding_matrix], \n",
    "                                input_length=SEQUENCE_LENGTH, trainable=False))\n",
    "Skipgram_BiLSTM_model.add(Dropout(0.5))\n",
    "Skipgram_BiLSTM_model.add(Bidirectional(LSTM(100, dropout=0.2)))\n",
    "Skipgram_BiLSTM_model.add(Dense(1, activation='sigmoid'))\n",
    "Skipgram_BiLSTM_model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 训练两个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153795,
     "status": "ok",
     "timestamp": 1684247109356,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "PoTU8w1O342I",
    "outputId": "65ff2829-77d2-41b2-984d-c2970fadec67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "40/40 [==============================] - 11s 104ms/step - loss: 0.4859 - accuracy: 0.7631 - val_loss: 0.5463 - val_accuracy: 0.8182\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.3624 - accuracy: 0.8417 - val_loss: 0.3798 - val_accuracy: 0.8546\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.3323 - accuracy: 0.8575 - val_loss: 0.3494 - val_accuracy: 0.8652\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 3s 85ms/step - loss: 0.3208 - accuracy: 0.8620 - val_loss: 0.3422 - val_accuracy: 0.8665\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.3114 - accuracy: 0.8658 - val_loss: 0.3309 - val_accuracy: 0.8644\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 4s 91ms/step - loss: 0.3016 - accuracy: 0.8716 - val_loss: 0.2982 - val_accuracy: 0.8789\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 4s 93ms/step - loss: 0.2930 - accuracy: 0.8760 - val_loss: 0.3173 - val_accuracy: 0.8721\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2869 - accuracy: 0.8788 - val_loss: 0.2920 - val_accuracy: 0.8787\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2797 - accuracy: 0.8823 - val_loss: 0.3182 - val_accuracy: 0.8700\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2768 - accuracy: 0.8831 - val_loss: 0.2844 - val_accuracy: 0.8844\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.2677 - accuracy: 0.8874 - val_loss: 0.3079 - val_accuracy: 0.8763\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.2665 - accuracy: 0.8880 - val_loss: 0.3027 - val_accuracy: 0.8812\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.2601 - accuracy: 0.8899 - val_loss: 0.2859 - val_accuracy: 0.8874\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2520 - accuracy: 0.8968 - val_loss: 0.2806 - val_accuracy: 0.8857\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 3s 88ms/step - loss: 0.2484 - accuracy: 0.8967 - val_loss: 0.2782 - val_accuracy: 0.8894\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.2459 - accuracy: 0.8968 - val_loss: 0.2782 - val_accuracy: 0.8873\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 3s 88ms/step - loss: 0.2417 - accuracy: 0.8998 - val_loss: 0.2985 - val_accuracy: 0.8825\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2392 - accuracy: 0.9012 - val_loss: 0.2870 - val_accuracy: 0.8868\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2322 - accuracy: 0.9027 - val_loss: 0.2721 - val_accuracy: 0.8905\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2282 - accuracy: 0.9057 - val_loss: 0.2987 - val_accuracy: 0.8820\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 8s 118ms/step - loss: 0.4810 - accuracy: 0.7631 - val_loss: 0.4230 - val_accuracy: 0.8452\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.3536 - accuracy: 0.8466 - val_loss: 0.3704 - val_accuracy: 0.8639\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.3291 - accuracy: 0.8572 - val_loss: 0.3505 - val_accuracy: 0.8674\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.3153 - accuracy: 0.8626 - val_loss: 0.3466 - val_accuracy: 0.8714\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.3058 - accuracy: 0.8699 - val_loss: 0.3211 - val_accuracy: 0.8758\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.2982 - accuracy: 0.8724 - val_loss: 0.3112 - val_accuracy: 0.8801\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.2892 - accuracy: 0.8771 - val_loss: 0.2905 - val_accuracy: 0.8832\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 4s 92ms/step - loss: 0.2844 - accuracy: 0.8792 - val_loss: 0.2908 - val_accuracy: 0.8849\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2758 - accuracy: 0.8834 - val_loss: 0.2906 - val_accuracy: 0.8802\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2732 - accuracy: 0.8854 - val_loss: 0.2804 - val_accuracy: 0.8855\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2680 - accuracy: 0.8864 - val_loss: 0.2859 - val_accuracy: 0.8822\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 4s 90ms/step - loss: 0.2625 - accuracy: 0.8888 - val_loss: 0.2905 - val_accuracy: 0.8880\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2579 - accuracy: 0.8911 - val_loss: 0.2742 - val_accuracy: 0.8893\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2508 - accuracy: 0.8947 - val_loss: 0.2768 - val_accuracy: 0.8902\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2459 - accuracy: 0.8970 - val_loss: 0.2794 - val_accuracy: 0.8886\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 4s 89ms/step - loss: 0.2426 - accuracy: 0.8992 - val_loss: 0.2820 - val_accuracy: 0.8909\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.2389 - accuracy: 0.9013 - val_loss: 0.2675 - val_accuracy: 0.8929\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 3s 87ms/step - loss: 0.2353 - accuracy: 0.9028 - val_loss: 0.2678 - val_accuracy: 0.8901\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2291 - accuracy: 0.9042 - val_loss: 0.2698 - val_accuracy: 0.8931\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 4s 88ms/step - loss: 0.2246 - accuracy: 0.9067 - val_loss: 0.2729 - val_accuracy: 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff80c377580>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# fit model\n",
    "CBOW_BiLSTM_model.fit(x=x_train, y=y_train, epochs=20, batch_size=BATCH_SIZE,\n",
    "                      validation_data=(x_test, y_test), verbose=1,callbacks=[early_stop])\n",
    "Skipgram_BiLSTM_model.fit(x=x_train, y=y_train,epochs=20, batch_size=BATCH_SIZE,\n",
    "                          validation_data=(x_test, y_test), verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 混淆矩阵定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boaNgeVn5mlb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "def c_report(y_true, y_pred):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    acc_sc = accuracy_score(y_true, y_pred)\n",
    "    print(\"Accuracy : \"+ str(acc_sc))\n",
    "    return acc_sc\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5, \n",
    "               cmap=\"Blues\", cbar=False)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 评估两个模型(基于CBOW与基于SKipGram的BiLSTM模型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3070,
     "status": "ok",
     "timestamp": 1684247489788,
     "user": {
      "displayName": "SK TIAAN",
      "userId": "01952584446737432247"
     },
     "user_tz": -480
    },
    "id": "uQNID8pg9lWX",
    "outputId": "92fd7f2f-e6d7-4576-e760-979cefe2e579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "Evalution of CBOW_BiLSTM_Model\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.89      4961\n",
      "           1       0.93      0.83      0.88      5039\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.89      0.88      0.88     10000\n",
      "weighted avg       0.89      0.88      0.88     10000\n",
      "\n",
      "Accuracy : 0.882\n",
      "\n",
      "\n",
      "Evalution of Skipgram_BiLSTM_Model\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      4961\n",
      "           1       0.89      0.90      0.89      5039\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Accuracy : 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8922"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CBOW_preds = (CBOW_BiLSTM_model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "Skipgram_preds = (Skipgram_BiLSTM_model.predict(x_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "print(\"Evalution of CBOW_BiLSTM_Model\")\n",
    "c_report(y_test, CBOW_preds)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Evalution of Skipgram_BiLSTM_model\")\n",
    "c_report(y_test, Skipgram_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4hAm5O2-ZGx"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(CBOW_BiLSTM_model, open('Models/CBOW_BiLSTM_model.save', 'wb'))\n",
    "pickle.dump(Skipgram_BiLSTM_model, open('Models/Skipgram_BiLSTM_model.save', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyML53fvYTvRhYFSeKxLPbg4",
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
